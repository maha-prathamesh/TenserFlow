{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unsigned-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-secretariat",
   "metadata": {},
   "source": [
    "# Improving Computer Vision Accuracy using Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-cover",
   "metadata": {},
   "source": [
    "## SIMPLE DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "preliminary-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.4960 - accuracy: 0.8247\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 0.3737 - accuracy: 0.8651\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3350 - accuracy: 0.8772\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.3109 - accuracy: 0.8856\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.2921 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.3331 - accuracy: 0.8821\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-sunday",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-vacuum",
   "metadata": {},
   "source": [
    "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
    "\n",
    "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
    "\n",
    "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
    "\n",
    "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
    "\n",
    "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "average-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.4417 - accuracy: 0.8369\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 62s 1ms/sample - loss: 0.2954 - accuracy: 0.8906\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 61s 1ms/sample - loss: 0.2463 - accuracy: 0.9090\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 64s 1ms/sample - loss: 0.2179 - accuracy: 0.9182\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1912 - accuracy: 0.9282\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 2s 246us/sample - loss: 0.2610 - accuracy: 0.9101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2610365111768246, 0.9101]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "              tf.keras.layers.MaxPooling2D(2, 2),\n",
    "              tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "              tf.keras.layers.MaxPooling2D(2,2),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-hopkins",
   "metadata": {},
   "source": [
    "### Steps to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-distance",
   "metadata": {},
   "source": [
    "Step 1 : is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-remedy",
   "metadata": {},
   "source": [
    "Step 2 : Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
    "\n",
    "The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
    "The size of the Convolution, in this case a 3x3 grid\n",
    "The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
    "In the first layer, the shape of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-numbers",
   "metadata": {},
   "source": [
    "Step 3 : Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-steel",
   "metadata": {},
   "source": [
    "Step 4 : The same 128 dense layers, and 10 output layers as in the pre-convolution example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-technology",
   "metadata": {},
   "source": [
    "Step 5 : Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-george",
   "metadata": {},
   "source": [
    "# Visualizing the Convolutions and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-rachel",
   "metadata": {},
   "source": [
    "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rough-breathing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "infrared-burton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d_6/Relu:0' shape=(None, 26, 26, 64) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling2d_6/MaxPool:0' shape=(None, 13, 13, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_7/Relu:0' shape=(None, 11, 11, 64) dtype=float32>,\n",
       " <tf.Tensor 'max_pooling2d_7/MaxPool:0' shape=(None, 5, 5, 64) dtype=float32>,\n",
       " <tf.Tensor 'flatten_11/Reshape:0' shape=(None, 1600) dtype=float32>,\n",
       " <tf.Tensor 'dense_14/Relu:0' shape=(None, 128) dtype=float32>,\n",
       " <tf.Tensor 'dense_15/Softmax:0' shape=(None, 10) dtype=float32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIRST_IMAGE = 0\n",
    "SECOND_IMAGE = 99\n",
    "THIRD_IMAGE = 27\n",
    "CONVOLUTION_NUMBER = 1\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "brutal-religious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_6_input:0' shape=(None, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nominated-dietary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7fcc44d77310>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "activation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prompt-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(3,4)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-smith",
   "metadata": {},
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-excerpt",
   "metadata": {},
   "source": [
    "## change convolution filter from 64 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "private-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 32s 536us/sample - loss: 0.4756 - accuracy: 0.8274\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 36s 601us/sample - loss: 0.3168 - accuracy: 0.8845\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 36s 600us/sample - loss: 0.2716 - accuracy: 0.9006\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 37s 624us/sample - loss: 0.2427 - accuracy: 0.9111\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 36s 599us/sample - loss: 0.2190 - accuracy: 0.9183\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 113,386\n",
      "Trainable params: 113,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 2s 203us/sample - loss: 0.2516 - accuracy: 0.9086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.251630320417881, 0.9086]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "              tf.keras.layers.MaxPooling2D(2, 2),\n",
    "              tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "              tf.keras.layers.MaxPooling2D(2,2),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-grenada",
   "metadata": {},
   "source": [
    "## Remove Second Layer of Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "flying-interference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 54s 899us/sample - loss: 0.3769 - accuracy: 0.8655\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 51s 853us/sample - loss: 0.2571 - accuracy: 0.9071\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 52s 867us/sample - loss: 0.2129 - accuracy: 0.9210\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 51s 842us/sample - loss: 0.1794 - accuracy: 0.9339\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1497 - accuracy: 0.9455\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               1384576   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,386,506\n",
      "Trainable params: 1,386,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 0.2505 - accuracy: 0.9168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25051212109625337, 0.9168]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "              tf.keras.layers.MaxPooling2D(2, 2),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-coupon",
   "metadata": {},
   "source": [
    "## Adding more convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "nonprofit-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 43s 708us/sample - loss: 0.6344 - accuracy: 0.7667\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 41s 680us/sample - loss: 0.4321 - accuracy: 0.8416\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 41s 677us/sample - loss: 0.3821 - accuracy: 0.8599\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 36s 603us/sample - loss: 0.3466 - accuracy: 0.8716\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 36s 594us/sample - loss: 0.3248 - accuracy: 0.8783\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 3, 3, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 24,330\n",
      "Trainable params: 24,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 1s 135us/sample - loss: 0.3684 - accuracy: 0.8650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3684345170021057, 0.865]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "              tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "              tf.keras.layers.MaxPooling2D(2, 2),\n",
    "              tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "              tf.keras.layers.MaxPooling2D(2,2),\n",
    "              tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "              tf.keras.layers.MaxPooling2D(2,2),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "              tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "test_loss = model.evaluate(test_images, test_labels)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-vertical",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-marina",
   "metadata": {},
   "source": [
    "Simple DNN  : <br>\n",
    "Epoch 5/5<br>\n",
    "60000/60000 [==============================] - 4s 70us/sample - loss: 0.2921 - accuracy: 0.8932<br>\n",
    "10000/10000 [==============================] - 0s 45us/sample - loss: 0.3331 - accuracy: 0.8821<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-adolescent",
   "metadata": {},
   "source": [
    "Convolution with 2 layer of 64:<br>\n",
    "Epoch 5/5<br>\n",
    "60000/60000 [==============================] - 65s 1ms/sample - loss: 0.1912 - accuracy: 0.9282<br>\n",
    "10000/10000 [==============================] - 2s 246us/sample - loss: 0.2610 - accuracy: 0.9101<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-increase",
   "metadata": {},
   "source": [
    "Change convolution filter from 64 to 32: <br>\n",
    "Epoch 5/5<br>\n",
    "60000/60000 [==============================] - 36s 599us/sample - loss: 0.2190 - accuracy: 0.9183<br>\n",
    "10000/10000 [==============================] - 2s 203us/sample - loss: 0.2516 - accuracy: 0.9086<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-ladder",
   "metadata": {},
   "source": [
    "Removing 2nd layer of convolution : <br>\n",
    "Epoch 5/5<br>\n",
    "60000/60000 [==============================] - 51s 843us/sample - loss: 0.1497 - accuracy: 0.9455<br>\n",
    "10000/10000 [==============================] - 2s 228us/sample - loss: 0.2505 - accuracy: 0.9168<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-palmer",
   "metadata": {},
   "source": [
    "Adding 3rd layer of convolution : <br>\n",
    "Epoch 5/5<br>\n",
    "60000/60000 [==============================] - 36s 594us/sample - loss: 0.3248 - accuracy: 0.8783<br>\n",
    "10000/10000 [==============================] - 1s 135us/sample - loss: 0.3684 - accuracy: 0.8650<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
